# Product Search with Vector Embeddings

## Instructions to run locally and in AWS

### Steps to run locally

1.  Prerequisites: Ensure you have Python 3.9+ and MySQL Server installed.
    Install the required Python packages:
    ```bash
    pip install mysql-connector-python sentence-transformers scikit-learn numpy
    ```

2.  Database Setup:
    Run the SQL script to create the `products_vectors` table:
    ```bash
    mysql -u root -p < create_products_vectors.sql
    ```

3.  Generate Data:
    Run the script to create the product list (`products.csv`):
    ```bash
    python generate_products.py
    ```

4.  Embed and Store:
    Generate embeddings and store them in the database:
    ```bash
    python embed_products.py
    ```
    *Note: Update `DB_CONFIG` in the script if your MySQL credentials differ.*

5.  Search:
    Run the simulated Lambda handler to test the search:
    ```bash
    python lambda_handler.py
    ```
    (You can modify the `__main__` block to test different queries).

### Steps to run in AWS

1.  Infrastructure:
    *   Create an RDS MySQL instance and allow access from your IP (for setup) and Lambda security group.
    *   Create an AWS Lambda function (Python 3.9+).

2.  Deployment:
    *   This project relies on `sentence-transformers` and `numpy`, which are too large for a standard Lambda bundle.
    *   Option A (Docker): Create a Dockerfile, install dependencies, copy `lambda_handler.py`, and deploy the container image to Lambda.
    *   Option B (Layers): Create a Lambda Layer containing the dependencies and attach it to your function.
    *   Upload `lambda_handler.py` as the function code.
    *   Set the handler to `lambda_handler.lambda_handler`.

3.  Configuration:
    *   Set Environment Variables in Lambda: `DB_HOST`, `DB_USER`, `DB_PASSWORD`, `DB_NAME`.
    *   Update `lambda_handler.py` to read these variables using `os.environ`.

## Explanation of vector similarity logic and any assumptions

æ­¤ This project uses semantic search rather than keyword matching.

### Vector Similarity Logic
1.  Vector Generation: We use the pre-trained model `all-MiniLM-L6-v2` from the SentenceTransformers library. This model converts any text (product name) into a dense vector (a list of 384 numbers) that represents its semantic meaning.
2.  Cosine Similarity: To find similar products, we calculate the Cosine Similarity between the user's query vector and every product vector in the database.
    *   Formula: `similarity(A, B) = (A . B) / (||A|| * ||B||)`
    *   A score closer to 1.0 indicates high similarity (similar meaning).
    *   A score closer to 0.0 indicates no similarity.
3.  Process:
    *   The user's query ("running shoes") is embedded into a vector.
    *   The system compares this vector against all stored product vectors.
    *   The top 5 products with the highest similarity scores are returned.

### Assumptions
*   The product dataset is small enough to fit in memory for the `lambda_handler` to perform a full scan (brute-force comparison).
*   The database is accessible from the Lambda function (correct VPC/Security Group configuration).
*   Mock data generated by `generate_products.py` is sufficient for demonstration.
*   Scalability limitation: Fetching all vectors for every search is O(N). A production system should use a Vector Database (like Pinecone, Milvus).

## Edge case handling details

*   Empty Query: The `lambda_handler` checks if the `query` parameter is present. If it is empty or missing, it returns a `400 Bad Request` error.
*   No Results: If the database is empty or no products match (though cosine similarity always returns a score), the system handles the database fetch gracefully. If no products exist in the DB, it returns a 404 error.
*   Database Connectivity: Wrapped in a `try-except` block to catch connection errors and return a `500 Internal Server Error` with details.
*   Embedding Updates: If a product name changes, its vector must be re-generated and updated in the database.
*   Duplicate Entries: The embedding script handles duplicate product IDs using `ON DUPLICATE KEY UPDATE` to prevent errors during re-runs.
